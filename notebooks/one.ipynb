{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# exportdate = '20250216093424'\n",
    "exportdate = '20250411161808'\n",
    "\n",
    "filename = 'Wikibase+World-' + exportdate + '.xml'\n",
    "tree = ET.parse(filename)\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "item_map = {}\n",
    "\n",
    "for page in root.findall('{http://www.mediawiki.org/xml/export-0.11/}page'):\n",
    "    title = page.find('{http://www.mediawiki.org/xml/export-0.11/}title').text\n",
    "    if title.startswith('Item:'):\n",
    "        item_key = title.replace('Item:', '')\n",
    "        text = page.find('{http://www.mediawiki.org/xml/export-0.11/}revision').find('{http://www.mediawiki.org/xml/export-0.11/}text').text\n",
    "        # item_map[item_key] = {\"text\": text}\n",
    "        data = json.loads(text)\n",
    "        item_map[item_key] = {\"data\": data}\n",
    "\n",
    "# print(item_map['Q386'])\n",
    "\n",
    "def get_statements(data, key):\n",
    "    claims = data.get('claims', {})\n",
    "    statements = {}\n",
    "    if key in claims:\n",
    "        statements[key] = []\n",
    "        for claim in claims[key]:\n",
    "            # Filter out mainsnak-> snaktype somevalue or novalue\n",
    "            if claim['mainsnak']['snaktype'] == 'somevalue' or claim['mainsnak']['snaktype'] == 'novalue':\n",
    "                continue\n",
    "            if claim['mainsnak']['datavalue']['type'] == 'string':\n",
    "                statements[key].append(claim['mainsnak']['datavalue']['value'])\n",
    "            elif claim['mainsnak']['datavalue']['type'] == 'wikibase-entityid':\n",
    "                statements[key].append(claim['mainsnak']['datavalue']['value']['id'])\n",
    "            elif claim['mainsnak']['datavalue']['type'] == 'time':\n",
    "                time_value = claim['mainsnak']['datavalue']['value']['time']\n",
    "                if time_value.startswith('+'):\n",
    "                    time_value = time_value[1:]\n",
    "                # If the day is 00, set it to 01 so it is parsable\n",
    "                if time_value[8:10] == '00':\n",
    "                    time_value = time_value[:8] + '01' + time_value[10:]\n",
    "                # If month is 00, set it to 01 so it is parsable\n",
    "                if time_value[5:7] == '00':\n",
    "                    time_value = time_value[:5] + '01' + time_value[7:]\n",
    "                statements[key].append(pd.to_datetime(time_value))\n",
    "            elif claim['mainsnak']['datavalue']['type'] == 'quantity':\n",
    "                statements[key].append(int(claim['mainsnak']['datavalue']['value']['amount']))\n",
    "            else:\n",
    "                print('Unknown type')\n",
    "    return statements\n",
    "\n",
    "# Extract some statement values from the json\n",
    "for key in item_map:\n",
    "    data = item_map[key]['data']\n",
    "    # Ignore things that are not P3 => Q10\n",
    "    if 'P3' not in data.get('claims', {}):\n",
    "        continue\n",
    "    # Get the P3 value\n",
    "    p3 = get_statements(data, 'P3').get('P3', [])\n",
    "    if 'Q10' not in p3:\n",
    "        item_map[key].update({\"instance_of\": p3})\n",
    "    # TODO some of these statements we expect to only have 1 value? so dont get a list?\n",
    "    item_map[key].update({\"url\": get_statements(data, 'P1').get('P1', [])})\n",
    "    item_map[key].update({\"host\": get_statements(data, 'P2').get('P2', [])})\n",
    "    item_map[key].update({\"instance\": get_statements(data, 'P3').get('P3', [])})\n",
    "    item_map[key].update({\"start_date\": get_statements(data, 'P5').get('P5', [])})\n",
    "    item_map[key].update({\"status\": get_statements(data, 'P13').get('P13', [])})\n",
    "    item_map[key].update({\"links_to\": get_statements(data, 'P55').get('P55', [])})\n",
    "    item_map[key].update({\"links_from\": get_statements(data, 'P56').get('P56', [])})\n",
    "    item_map[key].update({\"version\": get_statements(data, 'P57').get('P57', [])})\n",
    "    item_map[key].update({\"properties\": get_statements(data, 'P58').get('P58', [])})\n",
    "    item_map[key].update({\"edits\": get_statements(data, 'P59').get('P59', [])})\n",
    "    item_map[key].update({\"users\": get_statements(data, 'P60').get('P60', [])})\n",
    "    item_map[key].update({\"users_active\": get_statements(data, 'P61').get('P61', [])})\n",
    "    item_map[key].update({\"pages\": get_statements(data, 'P62').get('P62', [])})\n",
    "    item_map[key].update({\"highest_item\": get_statements(data, 'P67').get('P67', [])})\n",
    "\n",
    "# Extract the en label and description of the items\n",
    "for key in item_map:\n",
    "    data = item_map[key]['data']\n",
    "    labels = data.get('labels', {})\n",
    "    item_map[key] = {\"label\": key, **item_map[key]}\n",
    "    if labels:\n",
    "        item_map[key].update({\"label\": labels.get('en', {}).get('value', key)})\n",
    "        # logging.info(f\"Updated label for {key}: {item_map[key]['label']}\")\n",
    "    descriptions = data.get('descriptions', {})\n",
    "    if descriptions:\n",
    "        item_map[key].update({\"description\": descriptions.get('en', {}).get('value', '')})\n",
    "    else:\n",
    "        item_map[key].update({\"description\": ''})\n",
    "\n",
    "# And then supplement some of the values with their labels..\n",
    "for key in item_map:\n",
    "    if 'host' in item_map[key]:\n",
    "        host = item_map[key]['host']\n",
    "        if host:\n",
    "            host_label = item_map.get(host[0], {}).get('label', host[0])\n",
    "            item_map[key].update({\"host_label\": host_label})\n",
    "            # logging.info(f\"Updated host_label for {key}: {host_label}\")\n",
    "\n",
    "# Remove everything that doesn't have instanceof containing the string `Q10` (Wikibase site)\n",
    "item_map = {k: v for k, v in item_map.items() if 'Q10' in v.get('instance', [])}\n",
    "# Remove everything that doesn't have anything in the properties list\n",
    "item_map = {k: v for k, v in item_map.items() if v.get('properties', [])}\n",
    "# And things that has properties[0] == 0 (As this is probably a wiki with wikibase installed, but not data)\n",
    "item_map = {k: v for k, v in item_map.items() if v.get('properties', [0])[0] != 0}\n",
    "\n",
    "print(item_map['Q384'])\n",
    "\n",
    "# print how many there are\n",
    "print(len(item_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all wikibases, by their status\n",
    "# Q54 online\n",
    "# Q57 offline permanently\n",
    "# Q72 offline indefinitely\n",
    "statusToString = {\n",
    "    'Q54': 'online',\n",
    "    'Q57': 'offline permanently',\n",
    "    'Q72': 'offline indefinitely'\n",
    "}\n",
    "\n",
    "status_map = {}\n",
    "with_status = 0\n",
    "for key in item_map:\n",
    "    status = item_map[key].get('status', [])\n",
    "    if status:\n",
    "        status = status[0]\n",
    "        if status not in status_map:\n",
    "            status_map[status] = 0\n",
    "        status_map[status] += 1\n",
    "        with_status += 1\n",
    "# convert the map keys\n",
    "status_map = {statusToString.get(k, k): v for k, v in status_map.items()}\n",
    "# We assume the rest are online\n",
    "status_map['unknown'] = len(item_map) - with_status\n",
    "print(status_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, remove everything that isnt online, or unknown\n",
    "item_map = {k: v for k, v in item_map.items() if v.get('status', ['Q54'])[0] in ['Q54', 'unknown']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplement with more info just for the graph\n",
    "\n",
    "# Define specific colors for certain host (using host, not host_label)\n",
    "specific_colors = {\n",
    "    'Q4': '#F00000',  # Wikimedia Foundation\n",
    "    'Q5': '#5be971',  # independently hosted Wikibase\n",
    "    'Q6': '#f11fbe',  # Wikimedia Cloud Services\n",
    "    'Q7': '#6283ca',  # The Wikibase Consultancy\n",
    "    'Q8': '#ADD8E6',  # Wikibase Cloud\n",
    "    'Q117': '#ff6347',  # WBStack\n",
    "    'Q118': '#5cd45b',  # Miraheze\n",
    "    'Q322': '#ffa500',  # WikiTide\n",
    "    'Q323': '#8a2be2',  # WikiForge\n",
    "    'Q1434': '#ff69b4',  # wikibase-docker\n",
    "}\n",
    "\n",
    "# Add a \"graph_group\" to the item_map based on the host, including an unknown group\n",
    "# Also add the graph_color based on the specific_colors, unknown is grey\n",
    "for key in item_map:\n",
    "    host = item_map[key].get('host', [])\n",
    "    if host and host[0] in specific_colors:\n",
    "        item_map[key].update({\"graph_group\": item_map[key].get('host_label', 'unknown')})\n",
    "        item_map[key].update({\"graph_color\": specific_colors[host[0]]})\n",
    "    else:\n",
    "        item_map[key].update({\"graph_group\": 'unknown'})\n",
    "        item_map[key].update({\"graph_color\": '#D3D3D3'})\n",
    "        if host:\n",
    "            print (f\"Unknown host for {host}\") # << you might wat to add a colour for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the item_map to a file (that could easily be loaded back up)\n",
    "def convert_timestamps(obj):\n",
    "    if isinstance(obj, pd.Timestamp):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(\"Type not serializable\")\n",
    "\n",
    "with open('wikibase_world_' + exportdate + '.json', 'w') as f:\n",
    "    json.dump(item_map, f, indent=4, default=convert_timestamps)\n",
    "    logging.info(f\"Exported {len(item_map)} items to wikibase_world_{exportdate}.json\")\n",
    "\n",
    "# Function that can load from a file to a map too\n",
    "# def load_item_map(filename):\n",
    "#     with open(filename, 'r') as f:\n",
    "#         return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a filtered list for the graph\n",
    "item_map_f = item_map.copy()\n",
    "for key in list(item_map_f.keys()):\n",
    "    # need 250 highest_item\n",
    "    if 'highest_item' in item_map_f[key]:\n",
    "        highest_item = item_map_f[key]['highest_item']\n",
    "        if highest_item and highest_item[0] != 0:\n",
    "            if highest_item[0] < 250:\n",
    "                del item_map_f[key]\n",
    "                continue\n",
    "    # Remove things that dont have more than 250 pages\n",
    "    if 'pages' in item_map_f[key]:\n",
    "        pages = item_map_f[key]['pages']\n",
    "        if pages and pages[0] < 250:\n",
    "            del item_map_f[key]\n",
    "            continue\n",
    "\n",
    "# Add directed edges to the graph\n",
    "for key in item_map_f:\n",
    "    # Add it as a node\n",
    "    G.add_node(key)\n",
    "    # Add the edges\n",
    "    if 'links_to' in item_map_f[key]:\n",
    "        for link in item_map_f[key]['links_to']:\n",
    "            G.add_edge(key, link, direction='to')\n",
    "    if 'links_from' in item_map_f[key]:\n",
    "        for link in item_map_f[key]['links_from']:\n",
    "            G.add_edge(link, key, direction='from')\n",
    "\n",
    "# Calculate the positions of the nodes\n",
    "pos = nx.spring_layout(G, k=0.5)  # Increase the value of k to spread out the nodes more\n",
    "\n",
    "# Push nodes with no edges outward slightly\n",
    "for node in G.nodes():\n",
    "    if G.degree(node) == 0:\n",
    "        pos[node] *= 1.1  # Push the node outward by 10%\n",
    "\n",
    "# Create the edge trace\n",
    "x_edges = []\n",
    "y_edges = []\n",
    "for edge in G.edges():\n",
    "    x_edges.extend([pos[edge[0]][0], pos[edge[1]][0], None])\n",
    "    y_edges.extend([pos[edge[0]][1], pos[edge[1]][1], None])\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=x_edges, y=y_edges,\n",
    "    line=dict(width=0.3, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines',\n",
    "    showlegend=False)\n",
    "\n",
    "# Create a trace for each group\n",
    "group_traces = {}\n",
    "group_counts = {}\n",
    "for node in G.nodes():\n",
    "    # Some things have been filtered out (like having no properties listed..)\n",
    "    if node not in item_map_f:\n",
    "        continue\n",
    "    group = item_map_f[node]['graph_group']\n",
    "    if group not in group_traces:\n",
    "        group_traces[group] = {\n",
    "            'x': [],\n",
    "            'y': [],\n",
    "            'hovertext': [],\n",
    "            'color': item_map_f[node]['graph_color'],\n",
    "            'size': []\n",
    "        }\n",
    "        group_counts[group] = 0\n",
    "    group_traces[group]['x'].append(pos[node][0])\n",
    "    group_traces[group]['y'].append(pos[node][1])\n",
    "    active_users = item_map_f[node].get('users_active', [0])[0] if item_map_f[node].get('users_active') else 0\n",
    "    properties = item_map_f[node].get('properties', [0])[0] if item_map_f[node].get('properties') else 0\n",
    "    hover_text = (\n",
    "        f\"{item_map_f[node]['label']}<br>\"\n",
    "        f\"URL: {item_map_f[node].get('url', [''])[0] if item_map_f[node].get('url') else ''}<br>\"\n",
    "        f\"Host: {item_map_f[node].get('host_label', ['']) if item_map_f[node].get('host_label') else ''}<br>\"\n",
    "        f\"Version: {item_map_f[node].get('version', [''])[0] if item_map_f[node].get('version') else ''}<br>\"\n",
    "        f\"Start Date: {item_map_f[node].get('start_date', [''])[0] if item_map_f[node].get('start_date') else ''}<br>\"\n",
    "        f\"Active Users: {active_users}<br>\"\n",
    "        f\"Users: {item_map_f[node].get('users', [''])[0] if item_map_f[node].get('users') else ''}<br>\"\n",
    "        f\"Properties: {properties}<br>\"\n",
    "        f\"Edits: {item_map_f[node].get('edits', [''])[0] if item_map_f[node].get('edits') else ''}<br>\"\n",
    "        f\"Pages: {item_map_f[node].get('pages', [''])[0] if item_map_f[node].get('pages') else ''}\"\n",
    "    )\n",
    "    group_traces[group]['hovertext'].append(hover_text)\n",
    "\n",
    "    size_active_users = 4\n",
    "    if active_users:\n",
    "        if active_users > 0 and active_users < 10:\n",
    "            size_active_users = active_users + 5\n",
    "        elif active_users >= 10 and active_users < 100:\n",
    "            size_active_users = 20\n",
    "        elif active_users >= 100 and active_users < 1000:\n",
    "            size_active_users = 30\n",
    "        elif active_users >= 1000:\n",
    "            size_active_users = 40\n",
    "\n",
    "    size_properties = 4\n",
    "    if properties:\n",
    "        if properties > 0 and properties < 10:\n",
    "            size_properties = properties + 5\n",
    "        elif properties >= 10 and properties < 100:\n",
    "            size_properties = 20\n",
    "        elif properties >= 100 and properties < 1000:\n",
    "            size_properties = 30\n",
    "        elif properties >= 1000:\n",
    "            size_properties = 40\n",
    "\n",
    "    group_traces[group]['size'].append(size_active_users)\n",
    "    # group_traces[group]['size'].append(size_properties)\n",
    "    group_counts[group] += 1\n",
    "\n",
    "# Create the node traces\n",
    "node_traces = []\n",
    "for group, data in group_traces.items():\n",
    "    group_name_with_count = f\"{group} ({group_counts[group]})\"\n",
    "    node_trace = go.Scatter(\n",
    "        x=data['x'], y=data['y'],\n",
    "        mode='markers',\n",
    "        hovertext=data['hovertext'],\n",
    "        hoverinfo='text',\n",
    "        showlegend=True,\n",
    "        legendgrouptitle=dict(text=\"By host\"),\n",
    "        name=group_name_with_count,  # Use the graph_group string with count as the legend label\n",
    "        marker=dict(\n",
    "            showscale=False,\n",
    "            size=data['size'],\n",
    "            color=data['color'],\n",
    "            line_width=1,\n",
    "            line_color='black'\n",
    "            ))\n",
    "    node_traces.append(node_trace)\n",
    "\n",
    "fig = go.Figure(data=[edge_trace] + node_traces,\n",
    "                layout=go.Layout(\n",
    "                    title='Wikibases, and links, sized by # of active users',\n",
    "                    titlefont_size=16,\n",
    "                    showlegend=True,\n",
    "                    hovermode='closest',\n",
    "                    margin=dict(b=20, l=5, r=5, t=40),\n",
    "                    annotations=[dict(\n",
    "                        text=\"\",\n",
    "                        showarrow=False,\n",
    "                        xref=\"paper\", yref=\"paper\")],\n",
    "                    xaxis=dict(showgrid=False, zeroline=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False),\n",
    "                    height=800  # Set the height to twice the default (400)\n",
    "                    ))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# save the figure as html\n",
    "fig.write_html(\"wikibase_graph.html\", include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Filter out the Wikibase Cloud host\n",
    "filtered_group_counts = {k: v for k, v in group_counts.items() if k != 'Wikibase Cloud'}\n",
    "\n",
    "filtered_labels = [f\"{k} ({v})\" for k, v in filtered_group_counts.items()]\n",
    "filtered_values = list(filtered_group_counts.values())\n",
    "\n",
    "# Prepare data for the pie charts\n",
    "labels = [f\"{k} ({v})\" for k, v in group_counts.items()]\n",
    "values = list(group_counts.values())\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'domain'}, {'type': 'domain'}]])\n",
    "\n",
    "# Add the first pie chart\n",
    "fig.add_trace(go.Pie(labels=labels, values=values, name=\"All Hosts\"), 1, 1)\n",
    "\n",
    "# Add the second pie chart\n",
    "fig.add_trace(go.Pie(labels=filtered_labels, values=filtered_values, name=\"Excluding Wikibase Cloud\"), 1, 2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Number of wikibases by host\",\n",
    "    height=500,\n",
    "    width=800,\n",
    "    annotations=[dict(text='All Hosts', x=0.18, y=1, font_size=12, showarrow=False),\n",
    "                 dict(text='Excluding Wikibase Cloud', x=0.9, y=1, font_size=12, showarrow=False)]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"wikibase_pie_host_wikibases.html\", include_plotlyjs='cdn')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Calculate the number of active users per host\n",
    "active_users_by_host = {}\n",
    "for key, value in item_map.items():\n",
    "    host_label = value.get('host_label', 'unknown')\n",
    "    active_users_count = value.get('users_active', [0])[0]\n",
    "    if host_label not in active_users_by_host:\n",
    "        active_users_by_host[host_label] = 0\n",
    "    active_users_by_host[host_label] += active_users_count\n",
    "\n",
    "# Update labels to include the total number of active users per host\n",
    "labels = [f\"{k} ({v})\" for k, v in active_users_by_host.items()]\n",
    "values = list(active_users_by_host.values())\n",
    "\n",
    "# Filter out the Wikimedia Foundation\n",
    "filtered_active_users_by_host = {k: v for k, v in active_users_by_host.items() if k != 'Wikimedia Foundation'}\n",
    "\n",
    "filtered_labels = [f\"{k} ({v})\" for k, v in filtered_active_users_by_host.items()]\n",
    "filtered_values = list(filtered_active_users_by_host.values())\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'domain'}, {'type': 'domain'}]])\n",
    "\n",
    "# Add the first pie chart\n",
    "fig.add_trace(go.Pie(labels=labels, values=values, name=\"All Hosts\"), 1, 1)\n",
    "\n",
    "# Add the second pie chart\n",
    "fig.add_trace(go.Pie(labels=filtered_labels, values=filtered_values, name=\"Excluding Wikimedia Foundation\"), 1, 2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Number of active users by host\",\n",
    "    height=500,\n",
    "    width=800,\n",
    "    annotations=[dict(text='All Hosts', x=0.18, y=1, font_size=12, showarrow=False),\n",
    "                 dict(text='Excluding WMF', x=0.82, y=1, font_size=12, showarrow=False)]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"wikibase_pie_host_active_users.html\", include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of properties per host\n",
    "properties_by_host = {}\n",
    "for key, value in item_map.items():\n",
    "    host_label = value.get('host_label', 'unknown')\n",
    "    properties_count = value.get('properties', [0])[0]\n",
    "    if host_label not in properties_by_host:\n",
    "        properties_by_host[host_label] = 0\n",
    "    properties_by_host[host_label] += properties_count\n",
    "\n",
    "# Filter out the Wikibase Cloud host\n",
    "filtered_properties_by_host = {k: v for k, v in properties_by_host.items() if k != 'Wikibase Cloud'}\n",
    "\n",
    "# Prepare data for the pie charts\n",
    "labels = [f\"{k} ({v})\" for k, v in properties_by_host.items()]\n",
    "values = list(properties_by_host.values())\n",
    "filtered_labels = [f\"{k} ({v})\" for k, v in filtered_properties_by_host.items()]\n",
    "filtered_values = list(filtered_properties_by_host.values())\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'domain'}, {'type': 'domain'}]])\n",
    "\n",
    "# Add the first pie chart\n",
    "fig.add_trace(go.Pie(labels=labels, values=values, name=\"All Hosts\"), 1, 1)\n",
    "\n",
    "# Add the second pie chart\n",
    "fig.add_trace(go.Pie(labels=filtered_labels, values=filtered_values, name=\"Excluding Wikibase Cloud\"), 1, 2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Number of properties by host\",\n",
    "    height=500,\n",
    "    width=800,\n",
    "    annotations=[dict(text='All Hosts', x=0.18, y=1, font_size=12, showarrow=False),\n",
    "                 dict(text='Excluding Wikibase Cloud', x=0.9, y=1, font_size=12, showarrow=False)]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"wikibase_pie_host_properties.html\", include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Extract the version information from item_map\n",
    "version_counts = {}\n",
    "for key, value in item_map.items():\n",
    "    version = value.get('version', ['unknown'])[0]\n",
    "    if version not in version_counts:\n",
    "        version_counts[version] = 0\n",
    "    version_counts[version] += 1\n",
    "\n",
    "# Prepare data for the pie charts\n",
    "versions = list(version_counts.keys())\n",
    "counts = list(version_counts.values())\n",
    "\n",
    "# Filter out the most common version (for example, '1.39.7')\n",
    "filtered_version_counts = {k: v for k, v in version_counts.items() if k != '1.39.7'}\n",
    "filtered_versions = list(filtered_version_counts.keys())\n",
    "filtered_counts = list(filtered_version_counts.values())\n",
    "\n",
    "# Group versions by the first two parts for both filtered and unfiltered data\n",
    "grouped_version_counts = {}\n",
    "for version, count in version_counts.items():\n",
    "    grouped_version = '.'.join(version.split('.')[:2])\n",
    "    if grouped_version not in grouped_version_counts:\n",
    "        grouped_version_counts[grouped_version] = 0\n",
    "    grouped_version_counts[grouped_version] += count\n",
    "\n",
    "grouped_version_counts_filtered = {}\n",
    "for version, count in filtered_version_counts.items():\n",
    "    grouped_version = '.'.join(version.split('.')[:2])\n",
    "    if grouped_version not in grouped_version_counts_filtered:\n",
    "        grouped_version_counts_filtered[grouped_version] = 0\n",
    "    grouped_version_counts_filtered[grouped_version] += count\n",
    "\n",
    "grouped_versions = list(grouped_version_counts.keys())\n",
    "grouped_counts = list(grouped_version_counts.values())\n",
    "grouped_versions_filtered = list(grouped_version_counts_filtered.keys())\n",
    "grouped_counts_filtered = list(grouped_version_counts_filtered.values())\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=3, specs=[[{'type': 'domain'}, {'type': 'domain'}, {'type': 'domain'}]])\n",
    "\n",
    "# Add the first pie chart\n",
    "fig.add_trace(go.Pie(labels=versions, values=counts, name=\"All Versions\", showlegend=False, textinfo='none'), 1, 1)\n",
    "\n",
    "# Add the second pie chart\n",
    "fig.add_trace(go.Pie(labels=filtered_versions, values=filtered_counts, name=\"Excluding 1.39.7\", showlegend=False, textinfo='none'), 1, 2)\n",
    "\n",
    "# Add the third pie chart\n",
    "fig.add_trace(go.Pie(labels=grouped_versions, values=grouped_counts_filtered, name=\"Grouped Versions\", showlegend=False, textinfo='none'), 1, 3)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Spread of Wikibases by Version\",\n",
    "    height=500,\n",
    "    width=800,\n",
    "    annotations=[dict(text='All Versions', x=0.12, y=1.05, font_size=12, showarrow=False),\n",
    "                 dict(text='Excluding 1.39.7', x=0.5, y=1.05, font_size=12, showarrow=False),\n",
    "                 dict(text='Excluding 1.39.7, Grouped as x.xx', x=0.95, y=1.05, font_size=12, showarrow=False)]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"wikibase_pie_versions.html\", include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a table underneath that just shows the grouped versions (including 1.39.7 thought) and the counts\n",
    "df = pd.DataFrame(list(grouped_version_counts.items()), columns=['Version', 'Count'])\n",
    "fig = go.Figure(data=[go.Table(header=dict(values=['Version', 'Count']),\n",
    "                               cells=dict(values=[df['Version'], df['Count']]))\n",
    "                     ])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wikibases with properties: \", len({k: v for k, v in item_map.items() if v.get('properties', [0])[0] > 0}))\n",
    "print(\"Wikibases with properties, and more than 10 pages: \", len({k: v for k, v in item_map.items() if v.get('properties', [0])[0] > 0 and v.get('pages', [0])[0] > 10}))\n",
    "print(\"Wikibases with properties, and more than 10 pages, and 1 or more active users: \", len({k: v for k, v in item_map.items() if v.get('properties', [0])[0] > 0 and v.get('users_active', [0])[0] >= 1 and v.get('pages', [0])[0] > 10}))\n",
    "print(\"Wikibases with properties, and more than 10 pages, and 2 or more active users: \", len({k: v for k, v in item_map.items() if v.get('properties', [0])[0] > 0 and v.get('users_active', [0])[0] >= 2 and v.get('pages', [0])[0] > 10}))\n",
    "print(\"Wikibases that link to other wikibases: \", len({k: v for k, v in item_map.items() if v.get('links_to')}))\n",
    "print(\"Wikibases that only link to non Wikimedia Foundation wikibases: \", len({k: v for k, v in item_map.items() if v.get('links_to') and not set(v.get('links_to')).intersection({'Q1', 'Q2'})}))\n",
    "# for k, v in item_map.items():\n",
    "#     if v.get('links_to') and not set(v.get('links_to')).intersection({'Q1', 'Q2'}):\n",
    "#         print(\" - \", k, v.get('label', k), v.get('links_to'))\n",
    "print(\"Wikibases that link to other wikibases, excluding Wikimedia Foundation: \", len({k: v for k, v in item_map.items() if v.get('links_to') and len(set(v.get('links_to')) - {'Q1', 'Q2'}) > 0}))\n",
    "# for k, v in item_map.items():\n",
    "#     if v.get('links_to') and len(set(v.get('links_to')) - {'Q1', 'Q2'}) > 0:\n",
    "#         print(\" - \", k, v.get('label', k), v.get('links_to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random suite ramblings and estimates below here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# show how many different versions there are across all wikibases\n",
    "versions = set()\n",
    "for key in item_map:\n",
    "    versions.update(item_map[key].get('version', []))\n",
    "print(\"Different versions: \", len(versions))\n",
    "# Convert the set to a sorted list\n",
    "sorted_versions = sorted(versions)\n",
    "\n",
    "# Display the sorted versions\n",
    "sorted_versions\n",
    "\n",
    "# and output how many wikibases are using each version\n",
    "version_counts = {}\n",
    "for key in item_map:\n",
    "    for version in item_map[key].get('version', []):\n",
    "        if version not in version_counts:\n",
    "            version_counts[version] = 0\n",
    "        version_counts[version] += 1\n",
    "\n",
    "# Display the version counts\n",
    "version_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
